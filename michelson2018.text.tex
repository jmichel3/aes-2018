% Template for 61st conference for non-peer-reviewed articled
\documentclass[convention,peer-reviewed]{aesconf}

% Graphics path
\graphicspath{{./}{figs/}}

% UTF-8 encoding is recommended but use one that works for you.
\usepackage[utf8]{inputenc}

% Highly recommended package for better looking text automatically.
\usepackage{microtype}

% Natbib is used for more control on citations. You can use other moderd
% bibliography packages but please try to match the provided style.
\usepackage[numbers,square]{natbib} 


% These are useful for different purposes.
\usepackage{color}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

% The full title of the paper
\title{Automatic Guitar Tablature Transcription from Audio using Inharmonicity Regressions
and Bayesian Classification}

% Put the authors in order here. The number in brackets define the corresponding affiliation.
\author[1]{Jonathan Michelson}
\author[2]{Richard Stern}

% Affiliations go here
\affil[1]{Electro-Harmonix / New Sensor Corporation}
\affil[2]{Carnegie Mellon University}

% Correspondece should include the corresponding author's name and e-mail address
\correspondence{Jonathan Michelson}{jonathan@ehx.com}

% These are used for headers. Anything that fits is okay. Please use proper punctuation.

% If there are many authors, please use the form "First author et al."
\lastnames{Michelson and Stern}

% Short title should describe your topic but not be too long.
\shorttitle{Tablature Transcription using Inharmonicity Regressions}


% This is required and draws the top title
\include{aestitle}

\begin{document}


\twocolumn[
\maketitle % MANDATORY! 

\begin{onecolabstract}
We propose two new methods to classify guitar strings using only audio with the goal of developing automated tablature transcription. The first method regresses the log-inharmonicities of guitar strings against their pitches to obtain characteristic linear trajectories, and assigns unseen notes to the strings whose means and variances of their trajectories maximize the probability of the measured inharmonicities of the notes. The second method, developed as a baseline against which to compare the first, characterizes the inharmonicity distribution of each fretboard position as a normal probability density, then similarly assigns unseen notes to the fretboard positions that maximize the likelihood of their observed inharmonicities. Results from classical and acoustic guitars in the standard Real World Corpus of guitar recordings show that our regression method generally performs better than the existing inharmonicity-based system for test recordings on which the system was not individually trained, as well as show that exploiting the linear structure of log-inharmonicity trajectories increases overall performance compared to our second Bayesian method. We also derive an inharmonicity compensation feature to allow for transcription of arbitrarily-tuned guitars, provided the tuning is known. Results from personally-recorded acoustic and electric guitars in various alternate tunings show that our tuning compensation is successful at predicting tuning-related inharmonicity changes.
\end{onecolabstract}
]

\section{Introduction}

The guitar is a popular musical instrument whose family comprises a diverse collection of stringed instruments. Despite their variation, the majority of guitar configurations exhibit six strings tuned to $E2$ ($82$ Hz), $A2$ ($110$ Hz), $D3$ ($147$ Hz), $G3$ ($196$ Hz), $B3$ ($247$ Hz), and $E4$ ($330$ Hz). Most guitars also typically have upwards of 20 frets, allowing for versatile musical passage realizations along the fretboard. A consequence of the tuning and extensive fretting locations is liberal pitch overlap between neighboring strings; multiple fretboard locations can usually be selected to produce a given pitch. This ambiguity renders conventional music scores, which represent passages as notes and chords, undesirable for students trying to learn more skillful placement of scales and arpeggios, or for enthusiasts trying to uncover the fretboard positions used on riffs recorded by virtuosic players. Figure~\ref{fig:score-tabs} illustrates this ambiguity.

Tablature (abbr. tab) is an alternative music notation that does not suffer from the one-to-many mapping of scores. The staff, instead of representing pitch as in classical notation, depicts a birds-eye view of the guitar neck from the perspective of the performer. Each of the six horizontal lines signify a corresponding string on the guitar, and numbers on each line specify the fret to be fingered. Tablature is also more popular than scores for a number of likely reasons~\cite{macrae2010}, such as accessibility (legibility does not require formal musical training) and transportability (tablature can be easily represented with ASCII characters). Accurate tablature transcription is a laborious task, however. Dependable tablature containing few errors typically require the careful listening of a seasoned musician to an audio recording, and possibly video recordings for additional consultation. Reliable automation of this transcription task would benefit the guitar student community by expediting quality tablature generation and unearthing fretboard positions of guitar riffs in songs for which accurate tablature does not yet exist. Recent technologies proposed to encourage proliferation of accessible tablature transcription systems, such as web framework Robotaba~\cite{burlet2013}, demonstrate a desire among MIR researchers to see this theoretically mature but practically nascent field mature into student-benefitting applications. 
\begin{figure}[!htbp] 
\centering
\includegraphics[scale=0.2]{figs/score-tabs}
\caption{Musical score and two identical realizations of that score in tablature notation. Figure taken from~\cite{barbanchoi2012}.}
\label{fig:score-tabs}
\end{figure}

An important feature in systems that transcribe tablature from audio input is inharmonicity. When an ideal string fixed at both ends is displaced, the restoring force that causes it to oscillate is its tension. However, for a real string with stiffness, its elasticity contributes to this restoring force~\cite{fletcher1962}, causing the frequencies of the modes of vibration to deviate from integer multiples of the fundamental. Instead, the solutions to the one-dimensional wave equation become: 
\begin{equation}
\label{eq:fk}
f_k = kf_{0}\sqrt{1+\beta k^2}
\end{equation}
where $f_k$ is the $k$th harmonic of fundamental $f_0$ and $\beta$ is the inharmonicity of the string, defined by
\begin{equation}
\beta = \frac{\pi^3 Q d^4}{64 T l^2}. \label{eq:beta}
\end{equation}
In words, the inharmonicity $\beta$ of a vibrating string is a dimensionless quantity that depends on the Young's modulus $Q$, diameter $d$, tension $T$, and vibrating length $l$ of the string, and which skews the $k$th partial of the string upwards in frequency according to equation~(\ref{eq:fk}). See Figure~\ref{fig:skew}. Because of this, it is highly discriminative and has successfully been used as the sole feature in a transcription system~\cite{barbanchoi2012} which we refer to as the existing inharmonicity-based method.
\begin{figure}[!htbp]
\centering
\includegraphics[scale=0.3]{skew}
\caption{Power spectrum of note D2 plucked on an electric guitar at string 5 and fret 5. Red dashed lines are drawn at integer multiples of the fundamental. The partial peaks begin visibly skewing rightward after the fifth harmonic.}
\label{fig:skew}
\end{figure}

This work builds on the existing method. We propose an alternative procedure to exploit the inharmonicity of an unseen note for string classification: assign to it the string whose pre-computed log-inharmonicity trajectory most probably predicts the estimated log-inharmonicity of the note. To contextualize its performance as well as that of its predecessor, we also introduce a baseline Bayesian classifier of fretboard position given an unseen note: the measured inharmonicities of each fretboard position are collected and fit to a normal distribution, and are used to return the string that maximizes the probability of having observed the measured inharmonicity of the unseen note. To the author's knowledge, no results for strictly inharmonicity-based systems have yet been reported from a simple classifier for the sake of benchmarking. 

\section{Previous Work}

Empirical estimation of inharmonicity has been under investigation for nearly 40 years. Using pitch extraction techniques to perform estimation was first attempted by Galembo in 1979 and 1986~\cite{galembo1979,galembo1987}. Several additional methods have been proposed since then. In 1994, Galembo~\cite{galembo1994} hypothesized a connection between partials-based fundamental pitch estimates and the degree of inharmonicity in a spectrum. They specifically investigated cepstral analysis and a variant of the harmonic product spectrum method applied to both synthetic tones and recorded piano notes. The same authors in~\cite{galembo1999} introduced another method in 1999, dubbed the inharmonic comb filter (ICF) method. The ICF whose inharmonicity best matched that of the note would produce the output with lowest spectral energy, since its notches would most align with, and therefore best cancel, the inharmonic note partials. Rauhala~\cite{rauhala2007} introduced an efficient procedure in 2007, known as partial frequency deviations (PDF), that iteratively catalogued estimates of the deviations of partial frequencies of a note and returned increasingly better estimates of the inharmonicity. In 2009, Hodgkinson~\cite{hodgkinson2009} proposed a yet more efficient and accurate algorithm, named median-adjustive trajectories (MAT). Three years later, Barbancho~\cite{barbanchoi2012} introduced another PFD-based approach, which we refer to as the partial coincidence tally (PCT) method, as part of a guitar tablature transcription system. Also in 2012, Abesser~\cite{abesser2012} used parametric spectral modeling, again in the context of a guitar tablature transcription system.

Tablature transcription is also a rich research topic. A popular framework for the stringed-instrument transcription problem is that of weighted directed graphs that capture all plausible sequences of fretboard positions in a given musical passage~\cite{sayegh1989,radicioni2005,yazawa2013,burlet2013,burlet2015,radisav2004}. A number of teams have had success with machine learning techniques such as neural networks, genetic algorithms, hidden Markov models, and SVMs ~\cite{gagnon2003,tuohy2006,barbancho2009,barbanchoa2012,abesser2012,kehling2014,dittmar2013,abesser2012}. Some researchers have explored usage of media other than audio or musical information (i.e., scores), such as augmented guitar hardware and video recordings~\cite{ogrady2009,paleari2008,kerd2007}.

\section{Methods}

\subsection{Inharmonicity Regression}
Inharmonicity varies in a deterministic manner with respect to fret positions along a given string~\cite{barbanchoi2012}. Specifically, the inharmonicity $\beta(s,n)$ of a note produced by string $s$ at fret number $n$ can be expressed in terms of the inharmonicity of the open-string note $\beta(s,0)$ according to:
\begin{equation} 
\label{eq:beta-traj}
\beta(s,n) = \beta(s,0)2^{\frac{n}{6}}
\end{equation}
In other words, the variation in inharmonicity along a given string can be expressed simply in terms of its open-note inharmonicity, effectively defining a restricted trajectory along which the inharmonicity must vary. Figure~\ref{fig:beta-trajectories-ag} illustrates these trajectories. We propose characterizing each candidate string by its inharmonicity trajectory and classifying a note as simply belonging to the string whose inharmonicity trajectory evaluated at the fundamental of the unseen note best approximates the inharmonicity of the note.

\begin{figure}[h] 
\centering
\includegraphics[scale=0.35]{figs/beta-trajectories-ag}
\caption{Example inharmonicity trajectories of acoustic guitar strings, calculated from measured open-string inharmonicities. Only Fret 0 (open-string) through Fret 3 are shown. Observe the restriction to the exponential trajectory defined by equation~\eqref{eq:beta-traj}.}
\label{fig:beta-trajectories-ag}
\end{figure}

First, we estimate the inharmonicity of an unseen note. The algorithm we employ is the median adjustive trajectories (MAT) method~\cite{hodgkinson2009} proposed by Hodgkinson in 2009. The MAT algorithm was chosen for its efficiency and accuracy over the algorithms that preceded it~\cite{hodgkinson2009}. We apply this estimation procedure to five Hann-windowed audio segments of length $2^{14}$ samples, beginning at the detected onset of a note and at four successive 100-ms intervals (i.e. 0 ms, 100 ms, 200 ms, 300 ms, and 400 ms after the note onset). This yields five inharmonicity estimates we later compile into one classification result using the frame aggregation refinement technique in~\cite{abesser2012}: 

Next, we apply a log transformation to the estimated inharmonicities to make their variation linear with respect to MIDI pitch. If we reconsider equation~\eqref{eq:beta-traj} in terms of MIDI pitch number $m$ instead of fret number $n$, we obtain
\begin{equation}
\beta(s,m) = \beta(s,m_{os})2^{\frac{m-m_{os}}{6}},
\end{equation}
where $m_{os}$ is the open-string pitch of string $s$. Figure~\ref{fig:beta-v-midi} illustrates the trajectories in terms of MIDI pitches.
\begin{figure}[h] 
\centering
\includegraphics[scale=0.35]{beta-v-midi}
\caption{Inharmonicity estimates for Frets 0-12 on Strings 1-6 of RWC Acoustic Guitar AG111. Note the exponential trajectory as in Fig.~\ref{fig:beta-trajectories-ag} and the increased segregation since notes and their inharmonicities are now plotted against their fundamental instead of their fret number.}
\label{fig:beta-v-midi}
\end{figure}
Now if we consider the log-inharmonicity $\beta_{l}(m)$, we see that
\begin{equation}
\beta_l(m) = \log_2\beta(s,m) = \log_2[\beta(m_{os})2^{\frac{m-m_{os}}{6}}]
\end{equation}
\begin{equation}
\beta_l(m) = \log_2\beta(m_{os}) + \frac{m-m_{os}}{6}
\end{equation}
\begin{equation}
\beta_l(m) = (\log_2\beta(m_{os})-\frac{m_{os}}{6}) + (\frac{1}{6})m,
\end{equation}
where string $s$ has been dropped from the notation since we're looking only at variation along a fixed string. Substituting $w_0$ for $\log_2\beta(m_{os})-\frac{m_{os}}{6}$ and letting $w_1 = \frac{1}{6}$, we obtain the standar linear form
\begin{equation}
\label{eq:linear-traj}
\beta_l(m) = w_0 + w_1m,
\end{equation}
showing us that the log-inharmonicity trajectory of a given string varies linearly with respect to both MIDI pitch and fret number. See Figure~\ref{fig:log-beta-v-midi}.
\begin{figure}[!htbp] 
\centering
\includegraphics[scale=0.35]{log-beta-v-midi}
\caption{Log-inharmonicity estimates for same frets, strings, and guitar as Fig.~\ref{fig:beta-v-midi}.}
\label{fig:log-beta-v-midi}
\end{figure}

We next use linear regressions to obtaining empirical log-inharmonicity trajectories in order to train our system to understand the typical trajectories of guitar strings. We collect all notes with common string labels in our training data, and we perform linear regression of their log-inharmonicities $\beta$ against their fundamental pitches $m$ (in MIDI note number format). We do this for each string label. Let $s \in \{1,2,3,4,5,6\}$ be the string label to which each training note is assigned, and $N_s$ be the number of notes we have belonging to string label $s$. If we let $\mathbf{x}_s^{(i)} = [1, m_s^{(i)}]^T$ represent the $i$th note with fundamental pitch $m^{(i)}_s$ belonging to string $s$ , and let $\beta_s^{(i)}$ denote the log-inharmonicity of the $i$th note belonging to string $s$, we can solve
\begin{equation}
\label{lin-reg}
\mathbf{w}_s = \argmin_{\mathbf{w}}{\sum_{i=1}^{N_s}{(\beta^{(i)}_s - \mathbf{w}^T\mathbf{x}^{(i)}_s)^2}}
\end{equation}
which yields the weight vector $\mathbf{w}_s$ that minimizes the sum of squared error between the measured and predicted inharmonicities of the notes belonging to string $s$.
%\begin{figure}[!htbp] 
%\centering
%\includegraphics[scale=0.75]{traj-v-reg}
%\caption{Comparison of string inharmonicity characterization approaches for our recorded Fender Telecaster. Top: \textit{theoretical} trajectories. Note how each line begins precisely at the first marker of each string (i.e. the open-string note) and continues along a trajectory that ignores empirical inharmonicity estimates. String 1 most clearly shows this. Additionally, the trajectories for String 3 and String 4 are effectively co-linear and therefore indiscriminable. Bottom: regressions, or \textit{empirical} trajectories. Each regression was obtained by minimizing the bisquare weighted error of the residuals of the string, so the lines more closely follow the empirical inharmonicity scatterplot estimates.}
%\label{fig:traj-v-reg}
%\end{figure}

Occasional estimates from the inharmonicity measurement routine are outliers that deviate substantially from the linear trajectory model, so instead of the standard unweighted regression in~\eqref{lin-reg} we rather perform a bisquare weighted linear regression to approximate the log-inharmonicities. Bisquare weighting is a robust linear regression procedure that minimizes a \textit{weighted} sum of squares which de-emphasizes outliers~\cite{matlab-robustfit}. This provides us with higher-quality regressions that were more robust to stray inharmonicity measurements.

To classify unknown notes, we impose Gaussian probability distributions over the inharmonicity measurements of each string centered at their trajectories, and evaluate them at the inharmonicities of unseen notes to determine the most likely string from which it came. The means of the Gaussians are the log-inharmonicity values of points on the line, and the variances of the Gaussians are those of the residuals of the regressions, $\sigma_s^2$. In this manner, the probability distribution that characterizes the inharmonicity trajectory $\mathbf{w}_s$ of string $s$ at the fundamental pitch $m_0^{(i)}$ of the $i$th note $\mathbf{x}^{(i)} = [1,m_0^{(i)}]^T$ is defined as the normal distribution $\mathcal{N}(\mu, \sigma^2)$, with mean $\mu = \mathbf{w}_s^T\mathbf{x}^{(i)}$ and variance $\sigma^2 = \sigma_s^2$. Now to classify the $n$th unseen note $\mathbf{x}^{(n)}$, we simply measure its log-inharmonicity $\beta^{(n)}$ and obtain the probability that this log-inharmonicity was observed given each string $s$ according to
\begin{equation}
P(\mathbf{x}^{(n)},\beta^{(n)} | s) = \mathcal{N}(\beta^{(n)} | \mathbf{w}_s^T\mathbf{x}^{(n)},\sigma_s^2),
\end{equation}
and return the predicted string $\hat{s}^{(n)}$ that maximizes this probability,
\begin{equation}
\hat{s}^{(n)} = \argmax_{s}P(\mathbf{x}^{(n)},\beta^{(n)} | s).
\label{eq:string-argmax}
\end{equation}
We also attempted classification using a decision function that selected the trajectory which minimized the residual instead of maximizing the probability, but we found that the probabilistic method performed better.

Then, we transform the string classifier output into tablature notation. This is a trivial task provided we know the tuning $\mathbf{t} = [m_1, m_2, m_3, m_4, m_5, m_6]^T$ of the unknown guitar, where $m_s$ is the MIDI pitch number of the open note on string $s$. We already have an estimate of the MIDI pitch $m^{(n)}$ of the $n$th unknown note, so simply taking the difference between $m^{(n)}$ and the open pitch $m_s$ of its assigned string yields the fret on which the note was played; each fret on a guitar increases the pitch of the string by one half-step, or equivalently the MIDI pitch number by one integer.

Lastly, we refine the fretboard estimates with ``plausibility filtering"~\cite{abesser2012}, in which we reject and reassign string decisions that are implausible given the constraints and assumptions of the data. If our systems assigns to a note a string on which the corresponding fretted position is negative, a mistake has been made since frets cannot possibly be lower than 0 (the open-string). Similarly, if our system assigns to a note a string on which the corresponding fret is greater than 12, a mistake has also been made because our guitar recordings feature only frets 0-12. When either of these situations arise, we reject the string classification decision and select the next most probable string. If the succeeding string is also implausible, the process is iterated until all six strings have been considered.

\subsection{Baseline Bayesian Classifier}
The hypothesis of our first method is that exploitation of the linear trajectories of log-inharmonicities is helpful for transcription. To objectively evaluate this we develop a baseline classifier, which we simply call Bayesian classification, which \textit{does not} leverage the inharmonicity trajectories and whose results serve as a benchmark.

As before, we first measure the inharmonicities of guitar notes using median-adjustive-trajectories (MAT). Frame aggregation is performed using the same specifications discussed in Section~\ref{sec:string-classification}. Next, we estimate the inharmonicity distribution of each fretboard position $f$. For each of the $F=78$ positions (13 notes $\times$ 6 strings), we calculate the median $\tilde{\beta}_f$ and variance $\sigma^2_f$ of its inharmonicities, and use these to parameterize a Gaussian probability density $\mathcal{N}_f(\tilde{\beta}_f,\sigma^2_f)$. We choose the median to lessen the influence of outliers.

We then classify unseen notes. We measure the inharmonicity $\beta^{(i)}$ of the $i$th unseen note $\mathbf{x}=[1,m_0^{(i)}]^T$ with fundamental MIDI pitch $m_0^{(i)}$ and simply evaluate the probability $P_f(\beta^{(i)} | f) = \mathcal{N}_f(\beta^{(i)} | \tilde{\beta}_f,\sigma^2_f)$ of it having been generated by the Gaussian distribution $\mathcal{N}_f$. We narrow our search space $f \in \{1,2,3,...,F-1,F\}$ of possible fretboard positions by considering just the positions $f \in \{f_{m,1},f_{m,2},...\}$ that agree with the fundamental frequency of the unseen note. For standard-tuning, this is a maximum of only three positions. To classify a note, we select the candidate fretboard position which maximizes the likelihood of the measured inharmonicity of the note, just as in equation~\eqref{eq:string-argmax}:
\begin{equation}
\hat{f} = \argmax_{f\in\{f_{m,1},f_{m,2}...\}}P(\beta^{(i)} | f).
\label{eq:string-classification-mle}
\end{equation}


\subsection{Tuning Compensation Feature}
Though standard tuning is the most common pitch configuration of six-string guitars, there exist numerous other tunings in which performers often play. Aside from altering the musicality of the instrument, alternate tunings complicate the transcription process by introducing uncertainty about the open-string pitches. They distort inharmonicity-based methods, since the tensions of alternately-tuned guitar strings differ from those in standard-tuned strings, thereby affecting estimation of the inharmonicity. 

Current string classification systems do not address this degree of freedom. PCT~\cite{barbanchoi2012} is technically tuning-invariant, but requires access to recordings of the open strings of alternately-tuned test guitars. Other supervised learning systems~\cite{kehling2014, dittmar2013, abesser2012} are trained and evaluated only on standard-tuning guitars.

One approach to augment inharmonicity-based string classifiers with tuning-invariant performance is to introduce a scaling factor on the expected inharmonicity, or equivalently an additive factor on the expected log-inharmonicity. The fundamental frequency $f_0$ of an ideal vibrating string is related to its tension $T$ according to
\begin{equation}
f_0 = \frac{1}{2L}\sqrt{\frac{T}{\mu}}
\label{eq:freq-tension}
\end{equation}
where $L$ is string length and $\mu$ is its density. From this, we can see that
\begin{equation}
T \propto f_0^{2}.
\end{equation}
Recognizing that the equivalent change in frequency for a change in fundamental pitch of $\Delta m$ semitones is $2^{\frac{\Delta m}{12}}$, we see that the proportional change in tension (with all other factors constant) is
\begin{equation}
T \propto 2^{\frac{\Delta m}{6}}f_0^2.
\end{equation}
The resulting log-inharmonicity of this new open-string note with pitch $m_{os}' = m_{os}+\Delta m$, with original open-string pitch $m_{os}$, is therefore
\begin{equation}
\log_2\beta(m_{os}') = \log_2[ \frac{\pi^3 Q d^4}{64 T l^2}(2^{-\frac{\Delta m}{6}})].
\end{equation}
\begin{equation}
\label{eq:delta-beta}
\log_2\beta(m_{os}') = \log_2\frac{\pi^3 Q d^4}{64 T l^2} - \frac{\Delta m}{6}.
\end{equation}
Substituting~\eqref{eq:delta-beta} into the log-inharmonicity term of $w_0$ from equation~\eqref{eq:linear-traj}, we can get the new intercept term $w_0':$
\begin{equation}
w_{0}' = \log_2{\beta}(m_{os}') - \frac{m_{os}'}{6}
\end{equation}
\begin{equation}
w_{0}' = (\log_2\frac{\pi^3 Q d^4}{64 T l^2} - \frac{\Delta m}{6}) - \frac{m_{os}+\Delta m}{6}
\end{equation}
\begin{equation}
\label{eq:3.17}
w_{0}' = \log_2\frac{\pi^3 Q d^4}{64 T l^2} - \frac{m_{os}}{6} - \frac{\Delta m}{3}
\end{equation}
\begin{equation}
\label{eq:3.18}
w_{0}' = w_0 - \frac{\Delta m}{3}.
\end{equation}
The slope coefficient $w_1 = -\frac{m}{6}$ is independent from ${\Delta m}$, so the affected log-inharmonicity trajectory of this alternately-tuned string is therefore
\begin{equation}
\label{eq:3.20}
\beta'(m) = w_0 + w_1m - \frac{\Delta m}{3}.
\end{equation}
Equation~\eqref{eq:3.20} states that the effect of an alternate tuning on a string is simply introduction of a bias term $\frac{-\Delta m}{3}$ to the log-inharmonicity, where $\Delta m$ is the semitone deviation from standard tuning. Compensating our trajectories to arbitrarily-tuned guitars should thus be straightforward, provided we are given the alternate tuning in which the unseen notes are being performed. Indeed, as Figure~\ref{fig:tuning-eg} shows, when one string on the standard-tuned electric guitar is modified using equation~\eqref{eq:3.20}, the predictive alternate-tuning regression aptly captures the trend of the alternate-tuning inharmonicity.
\begin{figure}[!htbp] 
\label{fig:tuning-eg}
\centering
\includegraphics[scale=0.35]{tuning-eg}
\caption{Effect of alternate tuning on log-inharmonicity trajectories of an electric guitar. The circles are log-inharmonicity estimates of Frets 0-12 on String 6 of RWC Electric Guitar EG131 in standard tuning, and the dashed line is their bisquare-weighted regression. The asterisks are log-inharmonicity estimates for the same frets, string, and guitar recorded two semitones down. The dotted line passing through them is the \textit{predicted} regression obtained by applying equation~\eqref{eq:3.20} to the dashed line.}
\end{figure}

\section{Results}  

We evaluate the performances of these two systems in three scenarios, comparing their performance to that of the existing method in~\cite{barbanchoi2012}: when the systems have been trained on recordings of only the test guitar, when the systems have been trained on recordings of multiple guitars including the test guitar (all of which were in the same classical, acoustic, or electric class), and when the systems have been trained and tested on separate sets of guitars. Results show that although the existing system excels in the first case, our regression method surpasses it in the second case on average, suggesting its greater generalizability. The results also show that our leveraging of the linear log-inharmonicity string trajectories yields performance gains over the baseline fretboard-position classifier.

\section{Discussion} 

The result is a slight boost in classification accuracy. For our system, we used five overlapping Hann-windowed frames, each $2^{14}$ samples long, which start at 100-ms intervals after the detected onset of a note. Onsets were obtained through automated detection for convenience, the details of which are beyond the scope of this work but are cited for reference~\cite{bello2005,dixon2006}. This frame-aggregation procedure is possible only because the sustains of the RWC notes are all quite long, and this is, of course, a limitation. The entire analysis spans approximately three-fourths of a second, which implies that the analyzed music passage cannot exceed rhythms greater than quarter notes at 80 bpm -- this is an admittedly unrealistic requirement of guitar audio, notorious for featuring licks played with rapidity that exceeds eighth and sixteenth notes at considerably higher tempos. We therefore emphasize the theoretical, proof-of-concept nature of this work.

\section{Summary} 
We began by introducing the guitar and an essential component of its musicality: the overlapping pitch ranges of its strings. We related this to tablature and its ability to uniquely specify fretboard positions for musical passages, then highlighted its popularity and its tedious manual annotation process. This led us to introduce automatic transcription systems and our goals for this work as an extension to the topic, which we reviewed in the following chapter.

An essential component of the musicality of the guitar is the overlapping pitch ranges of its strings. Tablature is a popular music notation which is well-suited to this phenomenon because it can uniquely specify fretboard positions for musical passages. However, its manual annotation process is tedious. Automatic transcription systems This led us to introduce automatic transcription systems and our goals for this work as an extension to the topic, which we reviewed in the following chapter.

We paid special attention to the partial coincidence tally (PCT) transcription method introduced by Barbancho in~\cite{barbanchoi2012}, which exploited only one feature known as inharmonicity in development of a successful system using only audio. We explained how inharmonicity affects the degree of upward skew of a note's partials, then discussed the intuition behind its discriminative power before introducing our novel approaches.

After reiterating one of Barbancho's key realizations -- that inharmonicity along a given string was deterministic -- we showed that the logarithm of these trajectories was linear with respect to MIDI pitch. We consequently proposed characterizing guitar strings as their log-inharmonicity trajectories and performing classification (and therefore implicitly tablature transcription) by probabilistically assigning to unknown notes the strings whose lines best approximated the notes' log-inharmonicities. Following this, we proposed a baseline Bayesian method against which we could evaluate the utility of our regression innovation, and which similarly operated by assigning to an unknown note the fretboard position which maximized the probability of having observed its measured inharmonicity. Lastly, we derived the effect of a tuning change on our regression method, and arrived at a general tuning-compensation feature applicable to any inharmonicity-based system.

Next, we reported results from experiments on a subset of the RWC music instruments database and a couple personal guitar recordings. Comparisons of our baseline and regression approaches against the PCT method revealed that, overall, the regression method achieves lower error rates when transcribing guitars on which it wasn't individually trained. It also outperformed our baseline, arguing that leveraging inharmonicity trajectories is a worthwhile pursuit. We also validated our inharmonicity compensation feature, reporting accuracy increases on personal recordings of alternately-tuned electric and acoustic guitars.

A cursory investigation of the cause for the poor electric guitar transcription performance revealed that inharmonicity estimation for the instruments is less reliable, and both the pickups of the electric guitars and the intrinsic variability of the measurement routine are not to blame. We briefly examined the effect of various factors, such as guitar string geometry (gauge, length, stiffness, winding) and digital effects processing (overdrive), on inharmonicity measurement reliability and found that their influences were considerable. This potentially helps to explain the high inter-guitar inharmonicity variance, but it is still unclear why intra-guitar inharmonicity variance remains high for some of the RWC guitar strings. We discussed limitations of this work, cautioning against misinformed application of our reported results to real-world guitar scenarios, and closed with future work, encouraging development of systems which further study and exploit both guitar string geometry and effects processing factors, as well as potential system-level improvements and additions.

In conclusion, characterizing guitar strings as log-inharmonicity trajectories against pitch and classifying notes by maximizing the observation probabilities of the trajectories is an effective approach to inharmonicity-based tablature transcription. Most notably, in addition to outperforming the benchmark probabilistic classifier introduced in this work, it overall surpasses the existing inharmonicity-based method for classical and acoustic guitars on which the system wasn't exclusively trained. Furthermore, adjusting inharmonicity estimates with the compensation factor introduced in this work is effective for improving transcription accuracy on alternately-tuned guitars.


\bibliographystyle{jaes}

% Reference to bibliography file.
\bibliography{refs}


\end{document}